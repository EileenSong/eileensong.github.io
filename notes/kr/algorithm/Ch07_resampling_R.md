---
layout: article
title: Resampling R
permalink: /notes/kr/algorithm/Ch07_resampling_r
key: notes
sidebar:
  nav: notes-kr
aside:
  toc: true
mermaid: true
mathjax: true
---


*í•´ë‹¹ í¬ìŠ¤íŒ…ì€ 'an introduction to statistical learning' ì±…ì˜ ì—°ìŠµë¬¸ì œë¥¼ Rë¡œ í•´ì„í•˜ë©° ê³µë¶€í•œ ê²ƒì„ ì •ë¦¬í•´ë‘ì—ˆìŠµë‹ˆë‹¤*


<br>

# Linear_5.4 Exercises

<br>

## 5ë²ˆ

<br>

5. In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.

<br>

### a
`Question`
(a) Fit a logistic regression model that uses income and balance to
predict default
<br>


`Answer`

ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì„ í™œìš©í•˜ì—¬ Default ë°ì´í„°ì…‹ì—ì„œ ê¸°ë³¸ê°’(default)ì˜ í™•ë¥ ì„ ì˜ˆì¸¡í•´ì•¼í•œë‹¤. ì¼ë‹¨ glm()í•¨ìˆ˜ë¥¼ ì¨ì„œ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ fití•œë‹¤. fití•œë‹¤ëŠ” ê²ƒì€ ë‹¨ì§€ ë°ì´í„°ì— í†µê³„ëª¨ë¸ì„ ì ìš©í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.


```r
library(ISLR)
set.seed(123)

model <- glm(default ~ income + balance, data=Default, family=binomial)
summary(model)
```
set.seed()ëŠ” Rì˜ ë‚œìˆ˜ ìƒì„±ê¸° ì´ˆê¸° ìƒíƒœë¥¼ ì„¤ì •í•œë‹¤. 

![Alt text](img/resam1.png)


ì¼ë‹¨ ë„ì¶œëœ ê°’ì„ ë³´ë©´ p-valueê°’ì´ ë§¤ìš° ì‘ì•„ ìœ ì˜í•˜ë‹¤ê³  íŒë‹¨ëœë‹¤.



```
ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ë¶„í• í•  ë•Œ set.seed(123)ì„ ì‚¬ìš©í•˜ë©´, ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë”ë¼ë„ í•­ìƒ ê°™ì€ ë°ì´í„° ë¶„í•  ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.
```

### b
`Question`

(b) Using the validation set approach, estimate the test error of this
model. In order to do this, you must perform the following steps:


i. Split the sample set into a training set and a validation set.

`Answer`

ì¼ë‹¨, ë°ì´í„° ì…‹ì„ í›ˆë ¨ ì„¸íŠ¸ì™€ ê²€ì¦ ì„¸íŠ¸ë¡œ ë‚˜ëˆˆë‹¤

```r
sample_size <- dim(Default)[1]
train_index <- sample(1:sample_size, sample_size*0.7)
train_set <- Default[train_index, ]
validation_set <- Default[-train_index, ]
```

Default ë°ì´í„° í”„ë ˆì„ì˜ í–‰ì˜ ê°œìˆ˜ë¥¼ sample_sizeì— ì €ì¥í•˜ê³ ,
1ë¶€í„° sample_sizeê¹Œì§€ì˜ ìˆ«ì ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ 70%ì— í•´ë‹¹í•˜ëŠ” í›ˆë ¨ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ë¥¼ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œí•œë‹¹.

ê·¸ë¦¬ê³ , ì„ íƒí•œ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” í–‰ì„ train_setìœ¼ë¡œ,
-train_indexì¸ ë‚˜ë¨¸ì§€ í–‰ì„ validation_setìœ¼ë¡œ ì§€ì •í–ˆë‹¤.



`Question`
ii. Fit a multiple logistic regression model using only the training observations. 




`Answer`

í›ˆë ¨ ì„¸íŠ¸ë§Œ ì‚¬ìš©í•˜ì—¬ ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì„ ë‹¤ì‹œ ì í•©
```r
model_train <- glm(default ~ income + balance, data=train_set, family=binomial)
```




`Question`
iii. Obtain a prediction of default status for each individual in
the validation set by computing the posterior probability of
default for that individual, and classifying the individual to
the default category if the posterior probability is greater
than 0.5.



`Answer`

ê²€ì¦ ì„¸íŠ¸ì—ì„œì˜ ê¸°ë³¸ê°’(default) ì˜ˆì¸¡

```r
probs <- predict(model_train, validation_set, type="response")
predictions <- ifelse(probs > 0.5, "Yes", "No")
```


ë¡œì§€ìŠ¤í‹±ì—ì„œëŠ” ì…ë ¥ë³€ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë°˜ì‘ë³€ìˆ˜ì˜ **í™•ë¥ **ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤.


ê²€ì¦ ë°ì´í„°ì…‹ì˜ ë°˜ì‘ ë³€ìˆ˜(default) ê°’ì„ ì˜ˆì¸¡í•˜ê³ , type="response"ë¥¼ ì¨ì„œ ì˜ˆì¸¡ëœ í™•ë¥  ê°’ì„ ë°˜í™˜í•œë‹¤. 


ê·¸ë˜ì„œ, ë°˜í™˜ëœ ê°’ì´ 0.5ë³´ë‹¤ í¬ë©´ yesë¡œ, ì‘ìœ¼ë©´ noë¡œ ë¶„ë¥˜í•˜ê¸°ë¡œ í•´ë³´ì•˜ë‹¤.




`Question`
iv. Compute the validation set error, which is the fraction of
the observations in the validation set that are misclassifed.



`Answer`
ê²€ì¦ ì„¸íŠ¸ì—ì„œì˜ set error ê³„ì‚°

```r
misclass_rate <- mean(predictions != validation_set$default)
misclass_rate
```

ì˜ˆì¸¡ê°’ê³¼ ê²€ì¦ ë°ì´í„°ì˜ ì‹¤ì œê°’ì´ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ í‰ê· ì„ ê³„ì‚°í•´ë³¸ë‹¤.


ê²°ê³¼: [1] 0.02633333

100ê°œì¤‘ ì•½ 2~3ê°œ ì •ë„ ì˜ëª» ë¶„ë¥˜ëœ ê²ƒìœ¼ë¡œ í™•ì¸ë˜ì—ˆë‹¤.


<br>





### c


`Question`

(c) Repeat the process in (b) three times, using three diferent splits
of the observations into a training set and a validation set. Comment on the results obtained.

<br>

`Answer`

- 1ë²ˆì§¸ ê²°ê³¼: [1] 0.02633333

- 2ë²ˆì§¸ ê²°ê³¼: [1] 0.026

- 3ë²ˆì§¸ ê²°ê³¼: [1] 0.02733333


ì–»ì€ ì˜¤ì°¨ê°€ ìœ ì‚¬í•˜ë‹¤. 0.26~0.28 ì‚¬ì´ì— ìœ„ì¹˜í•˜ë©°, ë³€ë™ì„±ì´ ë‚®ê¸° ë•Œë¬¸ì— ì•ˆì •ì ì¸ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤.




<br>

### d

`Question`
(d) Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable
for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a
dummy variable for student leads to a reduction in the test error
rate


`Answer`
í•™ìƒ dummy variable, income, balanceë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ê°’ì˜ í™•ë¥  ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ë¥¼ ê·¸ë ¤ì•¼í•œë‹¤.

validation set approachë¥¼ ì‚¬ìš©í•´ì„œ ì˜¤ë¥˜ ì¸¡ì •í•˜ê³ , ë‹¤ìŒìœ¼ë¡œ dummy ë³€ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” ê²ƒì´ test error rateë¥¼ ì¤„ì´ëŠ”ì§€ í™•ì¸í•´ë³´ì•„ì•¼ í•œë‹¤...



ì¼ë‹¨ ë‹¤ì‹œ fit í•´ì£¼ê³ ,

```r
model_student <- glm(default ~ income + balance + student, data=train_set, family=binomial)
```



ê²€ì¦ ì„¸íŠ¸ì—ì„œì˜ ê¸°ë³¸ê°’(default) ì˜ˆì¸¡

```r
probs_student <- predict(model_student, validation_set, type="response")
predictions_student <- ifelse(probs_student > 0.5, "Yes", "No")
```


ê²€ì¦ ì„¸íŠ¸ì—ì„œì˜ error rate ê³„ì‚°

```r
misclass_rate_student <- mean(predictions_student != validation_set$default)
misclass_rate_student
```


ê²°ê³¼: [1] 0.02733333


(b)ì™€ (c)ì˜ ê²°ê³¼ì™€ ë¹„ìŠ·í•˜ë‹¤. ì¦‰, student ë³€ìˆ˜ë¥¼ ì¶”ê°€í•´ë„ test errorë¥¼ not reduce. 



<br>

## 8ë²ˆ

<br> 
`Question`

We will now perform cross-validation on a simulated data set.


ì´ë²ˆì—ëŠ” êµì°¨ê²€ì¦ ê´€ë ¨ ë¬¸ì œë‹¤.



### a

(a) Generate a simulated data set as follows:

```
> set.seed(1)
> x <- rnorm(100)
> y <- x - 2 * x^2 + rnorm(100)
```

In this data set, what is n and what is p? Write out the model
used to generate the data in equation form.



`Answer`

- n: ë°ì´í„°ì˜ ê´€ì¸¡ì¹˜ ê°œìˆ˜

 -> x ì™€ yì˜ ê¸¸ì´: 100ì…ë‹ˆë‹¤.

- p: ì‚¬ìš©ëœ ì˜ˆì¸¡ ë³€ìˆ˜ì˜ ê°œìˆ˜

 -> xì™€ x^2: 2


- equation form: \( Y = X - 2X^2 + \epsilon \)



```
rnorm(100)ìœ¼ë¡œ ìƒì„±ëœ ëœë¤í•œ ê°’ì´ê¸° ë•Œë¬¸ì—, ì¸¡ì • ì˜¤ì°¨ê°€ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì˜¤ì°¨ í•­ëª© Ïµê°€ ìˆìŒ
```



### b

`Question`
(b) Create a scatterplot of X against Y . Comment on what you fnd.


`Answer`

```r
plot(x, y, main="Scatterplot of X against Y", xlab="X", ylab="Y")
```


![Scatterplot of X against Y](img/resam2.png)


linearí•˜ì§€ ì•Šê³ , 2ì°¨ ë°©ì •ì‹ì˜ í˜•íƒœë¥¼ ì˜ ë„ê³  ìˆê³ , ë³€ë™ì„±ë„ í¬ì§€ ì•Šì€ ê²ƒ ê°™ë‹¤.




<br>


### c


`Question`
(c) Set a random seed, and then compute the LOOCV errors that
result from ftting the following four models using least squares:


1. $Y = \beta_0 + \beta_1X + \epsilon$
2. $Y = \beta_0 + \beta_1X + \beta_2X^2 + \epsilon$
3. $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$
4. $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \beta_4X^4 + \epsilon$


Note you may fnd it helpful to use the data.frame() function
to create a single data set containing both X and Y .



`Answer`

ìœ„ ë„¤ ëª¨ë¸ì„ fití•˜ê³ , LOOCVì˜¤ë¥˜ ê³„ì‚°í•´ì•¼í•œë‹¤.
bootíŒ¨í‚¤ì§€ì˜ cv.glm()í•¨ìˆ˜ë¥¼ ì¨ì„œ ê³„ì‚°í•´ë³´ê² ë‹¤.

```r
library(boot)

data <- data.frame(x=x, y=y)

glm1 <- glm(y ~ x, data=data)
cv1 <- cv.glm(data, glm1)

glm2 <- glm(y ~ x + I(x^2), data=data)
cv2 <- cv.glm(data, glm2)

glm3 <- glm(y ~ x + I(x^2) + I(x^3), data=data)
cv3 <- cv.glm(data, glm3)

glm4 <- glm(y ~ x + I(x^2) + I(x^3) + I(x^4), data=data)
cv4 <- cv.glm(data, glm4)

cv1$delta[1], cv2$delta[1], cv3$delta[1], cv4$delta[1]
```


>ê²°ê³¼

- cv1$delta[1]

[1] 7.288162

- cv2$delta[1]

[1] 0.9374236

- cv3$delta[1]

[1] 0.9566218

- cv4$delta[1]

[1] 0.9539049
 

ëª¨ë¸1ë²ˆì´ ê°€ì¥ ë†’ì€ CV ì˜¤ì°¨ë¥¼ ê°€ì§„ë‹¤. ìœ„ì—ì„œ ë³´ì•˜ë“¯ ë°ì´í„°ëŠ” 2ì°¨ í•¨ìˆ˜ì˜ í˜•íƒœë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— 1ì°¨ í•¨ìˆ˜ì¸ linearëŠ” ì˜ ì„¤ëª…ì„ ëª»í•˜ëŠ” ê²ƒì„.

3ì°¨ì™€ 4ì°¨ëŠ” ê°™ì€ ê³¡ì„ ì´ê¸° ë•Œë¬¸ì— í° ì„±ëŠ¥ ì°¨ì´ë¥¼ ê°€ì ¸ì˜¤ì§„ ì•ŠëŠ” ê²ƒ ê°™ìŒ.



```
cv.glm() í•¨ìˆ˜ì˜ ê²°ê³¼ ê°ì²´ì¸ cvì—ì„œ deltaëŠ” êµì°¨ ê²€ì¦ ì˜¤ì°¨ ì¶”ì •ì¹˜ë¥¼ í¬í•¨í•˜ëŠ” ë²¡í„°ì„. 


- delta[1]: ì›ë˜ ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ì í•©ëœ ëª¨ë¸ì— ëŒ€í•œ ì˜ˆì¸¡ ì˜¤ì°¨. ë‹¨ìˆœí•œ K-ê²¹ êµì°¨ ê²€ì¦ ì˜¤ì°¨ë‚˜ LOOCV ì˜¤ì°¨ë¥¼ ë‚˜íƒ€ë‚¼ ë•Œ ì‚¬ìš©ë¨.

- delta[2]: êµì •ëœ ì˜ˆì¸¡ ì˜¤ì°¨ë¥¼ ë‚˜íƒ€ëƒ„. ëª¨ë¸ì˜ ìœ ì—°ì„±ì„ ê³ ë ¤í•˜ì—¬ êµì •ëœ ê²ƒìœ¼ë¡œ, ì¼ë°˜ì ìœ¼ë¡œ ë” ë³µì¡í•œ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë¨
```



<br>


### d

`Question`
(d) Repeat (c) using another random seed, and report your results.
Are your results the same as what you got in (c)? Why?


`Answer`

ë‹¤ë¥¸ seedë¥¼ ì¨ì„œ ë°˜ë³µí•´ ë³´ì•˜ë‹¤, ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- cv1$delta[1]

[1] 9.858301

- cv2$delta[1]

[1] 1.00441

- cv3$delta[1]

[1] 1.01803

- cv4$delta[1]

[1] 1.035601


cì™€ ìœ ì‚¬í•˜ê¸° 1ë²ˆ í•¨ìˆ˜ê°€ ì˜¤ì°¨ê°€ ì œì¼ í¬ë‹¤. ë°ì´í„° ëª¨ë¸ì´ 2ì°¨ë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, ì‹œë“œê°€ ë°”ë€Œì–´ë„ ê°™ì€ ê²°ê³¼ê°€ ë‚˜ì˜¤ëŠ” ê²ƒ ê°™ë‹¤.



<br>

### e

`Question`
(e) Which of the models in (c) had the smallest LOOCV error? Is
this what you expected? Explain your answer


`Answer`

ì‹¤ì œ ë°ì´í„° ëª¨ë¸ê³¼ ë°ì´í„° êµ¬ì¡°ê°€ ê°€ì¥ ì¼ì¹˜í•˜ê¸° ë•Œë¬¸ì— 2ë²ˆ ëª¨ë¸ì´ ê°€ì¥ ë‚®ì€ LOOCV ì˜¤ì°¨ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ë°–ì— ì—†ëŠ” ê²ƒ ê°™ë‹¤. 





<br><br><br>
ëğŸ™‚
<br><br><br>